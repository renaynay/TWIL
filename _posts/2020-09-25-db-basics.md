---
layout:        post
title:         Database Basics
date:          2020-09-25 17:08:00
summary:       How do databases really work?
categories:    blog
tags:          database
---

# So what *are* databases and how do they work?

* data storage approaches (2 main): access paterns, sequential and random io access paterns
* sequential = reading contiguous blocks of memory, monotonically from lower offsets to higher ones, sequential writes writing contiguous blocks of memory without issuing seeks in file system in between (means predictable)
* random = could go from lower offsets to higher ones, but no contiguous blocks of memory reading, impossible to predict

* sequential writes != sequential reads, data written closely does not necessarily mean they will be read together
* to write data sequential, you have to buffer it (keep in mem until you have enough things to write to disk)
* for sequential reads, you need to lay it out sequentially with pre-sorting

concept: mutability / immutability
* mutable data structures pre allocate memory and do in-place updates for data that they manage, lots of random IO especially with writes. no shadowing required (reading from multiple sources, resolving conflics before returning data to client. 
concurrent access must be guarded by system of locks
* immutable requires mem buffering to guarantee writes are sequential, files not modified on disk -> requires reading from multiple sources and merging this (shadowing) 


storage system designs: 

*log-structured merge trees (LSM Tree)* 
lives on the disk, less about in-mem stuff, more about things that are going onn ndisk, optimized for sequential writes, eliminates random IO on inserts, updates, delete ops. These are friendlier for modern storage systems like SSDs

to allow sequential writes, batch it in-memory tables, implemented using binary search trees usually. When size reaches a certain threshold, it writes to disk (this is called "flush")

reading data requires checking in-mem table and merging contents before returning the result

once written to disk, data is immutable (not touched anymore). they can only be deleted. 

LSM storage converts random writes into sequenntial by means of buffering


*Sorted String Tables*
(RocksDB, Apache Cassandra) 

persistent immutable maps from keys to values (keys are byte strings)
every value has timestamp, write times for inserts / updates (which are indistingusihalbe in these systems)

two parts: index block and data block
index block = contains keys mapped to offset of the data blocks, pointing to where data located on disk
data block = B tree usually or hash table, consists of sequentially written key-value pairs. each key appears only once obviously

finding a value by a key can be done quickly by searching for offset in index block by the key and then locating data with the offset, then returning it

whenever mem table that was buffering the data gets flushed to the SS table, the SS table is basically a snapshot of the data.

to read, you have to merge (combine many SS tables into one).

